{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import pipeline\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PROCESSED CORPUS INTO NOTEBOOK\n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = './corpus-processed'\n",
    "authors_data = {}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    args = filename.split(\",\")\n",
    "    if args[0] not in authors_data:\n",
    "        authors_data[args[0]] = []\n",
    "\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_lines = [line.rstrip('\\n') for line in file]\n",
    "            authors_data[args[0]].extend(file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dekker': 0, 'fletcher': 1, 'ford': 2, 'jonson': 3, 'massinger': 4, 'middleton': 5, 'rowley': 6, 'shakespeare': 7, 'webster': 8}\n",
      "['dekker', 'fletcher', 'ford', 'jonson', 'massinger', 'middleton', 'rowley', 'shakespeare', 'webster']\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT AUTHOR NAMES\n",
    "\n",
    "authors = list(authors_data.keys())\n",
    "author_ids = {}\n",
    "for i in range(len(authors)):\n",
    "    author_ids[authors[i]] = i\n",
    "print(author_ids)\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE CORPUS PER AUTHOR\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "author_data_tokenized = {}\n",
    "for author in authors:\n",
    "    author_data = authors_data[author]\n",
    "    tokenized_sentences = [tokenizer.tokenize(s) for s in author_data]\n",
    "    author_data_tokenized[author] = tokenized_sentences\n",
    "\n",
    "with open('./torch-cache/tokenized_author_data.json', 'w') as file:\n",
    "    json.dump(author_data_tokenized, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inputs: 20112\n"
     ]
    }
   ],
   "source": [
    "# GENERATE BERT INPUT SEQUENCES, MASKS, AND LABELS\n",
    "\n",
    "MAX_SEQUENCE_LEN = 128  # can go up to 512\n",
    "bert_inputs = []\n",
    "bert_inputs_readable = []\n",
    "bert_input_masks = []\n",
    "data_labels = []\n",
    "\n",
    "for author in authors:\n",
    "    sentences = author_data_tokenized[author]\n",
    "    label = author_ids[author]\n",
    "\n",
    "    current_input = [\"[CLS]\"]\n",
    "    for s in sentences:\n",
    "        if len(s) + len(current_input) + 1 <= MAX_SEQUENCE_LEN - 1:\n",
    "            current_input.extend(s)\n",
    "            current_input.append(\".\")\n",
    "        else:\n",
    "            current_input.append(\"[SEP]\")\n",
    "            mask = [1 for _ in range(len(current_input))]\n",
    "\n",
    "            while len(current_input) != MAX_SEQUENCE_LEN:\n",
    "                current_input.append(\"[PAD]\")\n",
    "                mask.append(0)\n",
    "\n",
    "            bert_inputs.append(tokenizer.convert_tokens_to_ids(current_input))\n",
    "            bert_inputs_readable.append(current_input)\n",
    "            bert_input_masks.append(mask)\n",
    "            data_labels.append(label)\n",
    "            current_input = [\"[CLS]\"]\n",
    "\n",
    "print(f\"Total number of inputs: {len(bert_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATA TO DISK\n",
    "\n",
    "import torch\n",
    "\n",
    "x_inputs = torch.tensor(bert_inputs)\n",
    "x_masks = torch.tensor(bert_input_masks)\n",
    "y_labels = torch.tensor(data_labels)\n",
    "\n",
    "torch.save(x_inputs, f'./torch-cache/x_inputs_s{MAX_SEQUENCE_LEN}.pt')\n",
    "torch.save(x_masks, f'./torch-cache/x_masks_s{MAX_SEQUENCE_LEN}.pt')\n",
    "torch.save(y_labels, f'./torch-cache/y_labels_s{MAX_SEQUENCE_LEN}.pt')\n",
    "\n",
    "with open('./torch-cache/authors.json', 'w') as file:\n",
    "    json.dump(authors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  100,  1037,  2395,  4607,  2957,  3804,  1997, 13370,  2894,  2085,\n",
      "         2003,  1996,  3467,  1997,  2256, 27648,  2081, 14013,  2621,  2011,\n",
      "         2023,  2365,  1997,  2259,  1998,  2035,  1996,  8044,  2008, 10223,\n",
      "         5596,  2588,  2256,  2160,  1999,  1996,  2784,  8945, 25426,  1997,\n",
      "         1996,  4153,  3950,  2085,  2024,  2256, 11347,  5391,  2007, 13846,\n",
      "        29586,  2015,  2256, 18618,  2608,  5112,  2039,  2005, 10490,  2256,\n",
      "         8665, 21862,  6824,  2015,  2904,  2000, 12831,  6295,  2256, 21794,\n",
      "        20691,  2000, 26380,  5761, 11844,  9425,  5999,  2162,  6045,  2232,\n",
      "        17966,  2010, 15968,  2392,  1998,  2085,  2612,  1997, 15986, 25007,\n",
      "        26261,  2098,  2015,  2000, 25966,  1996,  9293,  1997, 19725,  4748,\n",
      "        14028, 12086,  2002,  4880,  2869,  9152, 14905,  2135,  1999,  1037,\n",
      "         3203,  1005,  1055,  4574,  2000,  1996,  5869,  6895, 24918, 24820,\n",
      "         1997,  1037, 11320,  2618,   102,     0,     0,     0]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "test = torch.load('./torch-cache/x_inputs.pt', weights_only=True)\n",
    "test_2 = torch.load('./torch-cache/y_labels.pt', weights_only=True)\n",
    "print(test[0], test_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  4607,  2048, 24812,  2635,  9098,  2189,  9391,  2306,  1012,\n",
      "         4607,  2007,  2048,  3564,  2006,  1037,  3242,  3402,  9891,  1996,\n",
      "        11002,  1012,  2006,  2033,  2515,  2189,  5247,  2023,  2614,  1012,\n",
      "         2006,  2033,  2008,  5223,  2035,  8499,  1012,  2026, 24665, 20113,\n",
      "         2935,  1012,  2024,  2017,  2045,  2007,  2115, 26892, 17125,  1012,\n",
      "         5292,  8024,  2017,  7179,  1012,  2106,  2025,  1045, 14187,  2017,\n",
      "         2006,  2115,  3268,  2000,  3422,  2008,  3904, 22995,  1005,  1040,\n",
      "         2149,  1012,  2045,  2125,  2003,  1005,  1056,  2017,  2008, 12419,\n",
      "         2033,  2007,  2023,  5005,  1012,  4654, 13765,  3372,  2048, 24812,\n",
      "         1012,  2339,  2003,  2026,  2293,  1005,  1055,  2004,  4502,  1000,\n",
      "        14931,  2061, 11844,  1998,  7570, 18752,  2094,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(x_inputs[0], y_labels[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci2470 Python 3.12.4",
   "language": "python",
   "name": "csci2470"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
