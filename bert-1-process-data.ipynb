{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import pipeline\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PROCESSED CORPUS INTO NOTEBOOK\n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = './corpus-processed'\n",
    "authors_data = {}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    args = filename.split(\",\")\n",
    "    if args[0] not in authors_data:\n",
    "        authors_data[args[0]] = []\n",
    "\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_lines = [line.rstrip('\\n') for line in file]\n",
    "            authors_data[args[0]].extend(file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dekker': 0, 'fletcher': 1, 'ford': 2, 'jonson': 3, 'massinger': 4, 'middleton': 5, 'rowley': 6, 'shakespeare': 7, 'webster': 8}\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT AUTHOR NAMES\n",
    "\n",
    "authors = list(authors_data.keys())\n",
    "author_ids = {}\n",
    "for i in range(len(authors)):\n",
    "    author_ids[authors[i]] = i\n",
    "print(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE CORPUS PER AUTHOR\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "author_data_tokenized = {}\n",
    "for author in authors:\n",
    "    author_data = authors_data[author]\n",
    "    tokenized_sentences = [tokenizer.tokenize(s) for s in author_data]\n",
    "    author_data_tokenized[author] = tokenized_sentences\n",
    "\n",
    "with open('./torch-cache/tokenized_author_data.json', 'w') as file:\n",
    "    json.dump(author_data_tokenized, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inputs: 10058\n"
     ]
    }
   ],
   "source": [
    "# GENERATE BERT INPUT SEQUENCES, MASKS, AND LABELS\n",
    "\n",
    "MAX_SEQUENCE_LEN = 256  # can go up to 512\n",
    "bert_inputs = []\n",
    "bert_inputs_readable = []\n",
    "bert_input_masks = []\n",
    "data_labels = []\n",
    "\n",
    "# TODO: missing period between sentences!! oof\n",
    "\n",
    "for author in authors:\n",
    "    sentences = author_data_tokenized[author]\n",
    "    label = author_ids[author]\n",
    "\n",
    "    current_input = [\"CLS\"]\n",
    "    for s in sentences:\n",
    "        if len(s) + len(current_input) <= MAX_SEQUENCE_LEN - 1:\n",
    "            current_input.extend(s)\n",
    "        else:\n",
    "            current_input.append(\"[SEP]\")\n",
    "            mask = [1 for _ in range(len(current_input))]\n",
    "\n",
    "            while len(current_input) != MAX_SEQUENCE_LEN:\n",
    "                current_input.append(\"[PAD]\")\n",
    "                mask.append(0)\n",
    "\n",
    "            bert_inputs.append(tokenizer.convert_tokens_to_ids(current_input))\n",
    "            bert_inputs_readable.append(current_input)\n",
    "            bert_input_masks.append(mask)\n",
    "            data_labels.append(label)\n",
    "            current_input = [\"CLS\"]\n",
    "\n",
    "print(f\"Total number of inputs: {len(bert_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATA TO DISK\n",
    "\n",
    "import torch\n",
    "\n",
    "x_inputs = torch.tensor(bert_inputs)\n",
    "x_masks = torch.tensor(bert_input_masks)\n",
    "y_labels = torch.tensor(data_labels)\n",
    "\n",
    "torch.save(x_inputs, './torch-cache/x_inputs_256.pt')\n",
    "torch.save(x_masks, './torch-cache/x_masks_256.pt')\n",
    "torch.save(y_labels, './torch-cache/y_labels_256.pt')\n",
    "\n",
    "with open('./torch-cache/authors.json', 'w') as file:\n",
    "    json.dump(authors, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci2470 Python 3.12.4",
   "language": "python",
   "name": "csci2470"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
