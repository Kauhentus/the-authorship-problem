{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10058, 256])\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA FROM DISK\n",
    "\n",
    "x_inputs = torch.load('./torch-cache/x_inputs_256.pt', weights_only=True)\n",
    "x_masks = torch.load('./torch-cache/x_masks_256.pt', weights_only=True)\n",
    "y_labels = torch.load('./torch-cache/y_labels_256.pt', weights_only=True)\n",
    "\n",
    "print(x_inputs.shape)\n",
    "\n",
    "with open('./torch-cache/authors.json', 'r') as file:\n",
    "    authors = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT + LABEL SHAPES:  torch.Size([10058, 256]) torch.Size([10058, 256]) torch.Size([10058])\n",
      "9,052 training samples\n",
      "1,006 validation samples\n"
     ]
    }
   ],
   "source": [
    "# PUT DATA INTO TORCH DATALOADER\n",
    "\n",
    "print(\"INPUT + LABEL SHAPES: \", x_inputs.shape, x_masks.shape, y_labels.shape)\n",
    "\n",
    "dataset = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  100,  2002,  2003,  1996,  2062,  5506,  6178,  1997,  1996,  2048,\n",
      "         2017,  2079,  2025, 10439,  2890, 22342,  2033,  2017,  2031,  1037,\n",
      "         2980,  5317,  1999,  2115,  2677,  2085,  2017,  3685,  2907,  2292,\n",
      "         2033,  2041,  2007,  2009,  6203,  2663,  1045,  1005,  2222,  2425,\n",
      "         2032,  2870,  2079,  1998,  2202,  2035,  1996,  4283,  1998,  2172,\n",
      "         2204,  2079, 15177,  3492,  2540,  2663,  2909,  2026,  2388,  2038,\n",
      "         2018,  2014, 14085,  7730,  2300,  3459,  9906,  2011,  1996, 23626,\n",
      "         2273,  1999, 11190,  4644,  1998,  2027,  2031,  2409,  2014,  2014,\n",
      "         7280,  1998,  2079,  5676,  2014,  2016,  4618,  2196,  2031,  3407,\n",
      "         3178,  4983,  2016,  5914,  2306,  2023, 12411,  1005,  2305,  1998,\n",
      "         2043,  2009,  2003,  2009,  2442,  2022,  1037, 28441,  2027,  2360,\n",
      "         1037,  2100,  2021,  2009,  2442,  2022,  1037, 10170, 28441,  2748,\n",
      "         2061,  1996,  1056,  1005,  2060,  2158,  1997, 16808, 15155,  2758,\n",
      "         2021,  2515,  2016,  2903,  2068,  2748,  1998,  2038,  2042,  2012,\n",
      "         2793, 10278,  3807,  2144,  2296,  2154,  2000,  1999, 15549,  2890,\n",
      "         2065,  2151, 10170,  2022,  2045,  2030,  2000,  2272,  2045,  5506,\n",
      "         2339,  2023,  2003,  1037, 18179,  1037,  8210,  3538,  1997,  3218,\n",
      "         2588,  2014,  2011,  2122, 17727, 14122,  5668,  1045,  2425,  2014,\n",
      "         2061,  2030,  2842,  2360,  1045,  2008,  2027,  2812,  2070,  2402,\n",
      "         5506, 17695, 10170,  2005,  1996,  6548,  2064,  1041, 15549,  6767,\n",
      "        16280,  2004,  2092,  2004,  1037,  4497, 10684,  1998,  3568,  2052,\n",
      "         1045, 18012,  2017,  2000,  2022,  1037,  2210,  5506,  4063,  2084,\n",
      "         3040, 24209,  2906, 15534,  2182, 10354,  3334,  2073,  2003,  2016,\n",
      "        18385,  2664,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids = batch[0]\n",
    "    b_input_mask = batch[1]\n",
    "    b_labels = batch[2]\n",
    "\n",
    "    print(b_input_ids[0])\n",
    "    print(len(b_input_ids[0]))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# USE PRE-TRAINED BERT MODEL\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels=len(authors),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kauhe\\anaconda3\\envs\\authorship-problem\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SET UP HYPERPARAMETERS\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 4 # shoudl be 2-4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([2.3016, 0.8333, 3.1929, 0.7938, 1.2103, 0.8292, 4.5510, 0.3050, 3.8243],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "all_labels = np.concatenate([batch[2].numpy() for batch in train_dataloader])\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',  # Option to balance automatically\n",
    "    classes=np.unique(all_labels),\n",
    "    y=all_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    566.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    566.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    566.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    566.    Elapsed: 0:00:14.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:18.\n",
      "  Batch   240  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   280  of    566.    Elapsed: 0:00:25.\n",
      "  Batch   320  of    566.    Elapsed: 0:00:29.\n",
      "  Batch   360  of    566.    Elapsed: 0:00:32.\n",
      "  Batch   400  of    566.    Elapsed: 0:00:36.\n",
      "  Batch   440  of    566.    Elapsed: 0:00:40.\n",
      "  Batch   480  of    566.    Elapsed: 0:00:43.\n",
      "  Batch   520  of    566.    Elapsed: 0:00:47.\n",
      "  Batch   560  of    566.    Elapsed: 0:00:50.\n",
      "\n",
      "  Average training loss: 1.49\n",
      "  Training epcoh took: 0:00:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.72\n",
      "  Validation Loss: 0.91\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    566.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    566.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    566.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    566.    Elapsed: 0:00:14.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:18.\n",
      "  Batch   240  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   280  of    566.    Elapsed: 0:00:26.\n",
      "  Batch   320  of    566.    Elapsed: 0:00:29.\n",
      "  Batch   360  of    566.    Elapsed: 0:00:33.\n",
      "  Batch   400  of    566.    Elapsed: 0:00:36.\n",
      "  Batch   440  of    566.    Elapsed: 0:00:40.\n",
      "  Batch   480  of    566.    Elapsed: 0:00:44.\n",
      "  Batch   520  of    566.    Elapsed: 0:00:47.\n",
      "  Batch   560  of    566.    Elapsed: 0:00:51.\n",
      "\n",
      "  Average training loss: 0.58\n",
      "  Training epcoh took: 0:00:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.86\n",
      "  Validation Loss: 0.45\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    566.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    566.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    566.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    566.    Elapsed: 0:00:14.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:18.\n",
      "  Batch   240  of    566.    Elapsed: 0:00:21.\n",
      "  Batch   280  of    566.    Elapsed: 0:00:25.\n",
      "  Batch   320  of    566.    Elapsed: 0:00:28.\n",
      "  Batch   360  of    566.    Elapsed: 0:00:32.\n",
      "  Batch   400  of    566.    Elapsed: 0:00:36.\n",
      "  Batch   440  of    566.    Elapsed: 0:00:39.\n",
      "  Batch   480  of    566.    Elapsed: 0:00:43.\n",
      "  Batch   520  of    566.    Elapsed: 0:00:46.\n",
      "  Batch   560  of    566.    Elapsed: 0:00:50.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epcoh took: 0:00:50\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.91\n",
      "  Validation Loss: 0.30\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    566.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    566.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    566.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    566.    Elapsed: 0:00:14.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:18.\n",
      "  Batch   240  of    566.    Elapsed: 0:00:21.\n",
      "  Batch   280  of    566.    Elapsed: 0:00:25.\n",
      "  Batch   320  of    566.    Elapsed: 0:00:28.\n",
      "  Batch   360  of    566.    Elapsed: 0:00:32.\n",
      "  Batch   400  of    566.    Elapsed: 0:00:35.\n",
      "  Batch   440  of    566.    Elapsed: 0:00:39.\n",
      "  Batch   480  of    566.    Elapsed: 0:00:42.\n",
      "  Batch   520  of    566.    Elapsed: 0:00:46.\n",
      "  Batch   560  of    566.    Elapsed: 0:00:49.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:00:50\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:03:28 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "seed_val = 42\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        # loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE FINE-TUNED MODEL TO DISK\n",
    "\n",
    "torch.save(model, './torch-cache/test11_base_256.ckpt')\n",
    "\n",
    "# model_e2_b_cw1_s128 -> 0.80\n",
    "# model_e3_b_cw1_s128 -> 0.81\n",
    "# model_e4_b_cw1_s128 -> 0.85\n",
    "\n",
    "# model_e2_b_cw1_s256 -> 0.87\n",
    "# model_e3_b_cw1_s256 -> 0.91\n",
    "# model_e4_b_cw1_s256 -> 0.92\n",
    "\n",
    "# model_e2_b_cw1_s512 -> 0.80\n",
    "# model_e3_b_cw1_s512 -> 0.90\n",
    "# model_e4_b_cw1_s512 -> 0.96\n",
    "\n",
    "# model_e4_l_cw1_s256 -> 0.93\n",
    "# model_e4_l_cw1_s128 -> ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authorship-problem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
