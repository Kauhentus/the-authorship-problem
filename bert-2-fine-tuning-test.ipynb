{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19043, 128])\n",
      "['dekker', 'fletcher', 'ford', 'jonson', 'massinger', 'middleton', 'rowley', 'shakespeare', 'webster']\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA FROM DISK\n",
    "\n",
    "x_inputs = torch.load('./torch-cache/x_inputs.pt', weights_only=True)\n",
    "x_masks = torch.load('./torch-cache/x_masks.pt', weights_only=True)\n",
    "y_labels = torch.load('./torch-cache/y_labels.pt', weights_only=True)\n",
    "\n",
    "print(x_inputs.shape)\n",
    "\n",
    "with open('./torch-cache/authors.json', 'r') as file:\n",
    "    authors = json.load(file)\n",
    "    print(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  100,  1037,  2395,  4607,  2957,  3804,  1997, 13370,  2894,  2085,\n",
      "         2003,  1996,  3467,  1997,  2256, 27648,  2081, 14013,  2621,  2011,\n",
      "         2023,  2365,  1997,  2259,  1998,  2035,  1996,  8044,  2008, 10223,\n",
      "         5596,  2588,  2256,  2160,  1999,  1996,  2784,  8945, 25426,  1997,\n",
      "         1996,  4153,  3950,  2085,  2024,  2256, 11347,  5391,  2007, 13846,\n",
      "        29586,  2015,  2256, 18618,  2608,  5112,  2039,  2005, 10490,  2256,\n",
      "         8665, 21862,  6824,  2015,  2904,  2000, 12831,  6295,  2256, 21794,\n",
      "        20691,  2000, 26380,  5761, 11844,  9425,  5999,  2162,  6045,  2232,\n",
      "        17966,  2010, 15968,  2392,  1998,  2085,  2612,  1997, 15986, 25007,\n",
      "        26261,  2098,  2015,  2000, 25966,  1996,  9293,  1997, 19725,  4748,\n",
      "        14028, 12086,  2002,  4880,  2869,  9152, 14905,  2135,  1999,  1037,\n",
      "         3203,  1005,  1055,  4574,  2000,  1996,  5869,  6895, 24918, 24820,\n",
      "         1997,  1037, 11320,  2618,   102,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print(x_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT + LABEL SHAPES:  torch.Size([19043, 128]) torch.Size([19043, 128]) torch.Size([19043])\n",
      "17,138 training samples\n",
      "1,905 validation samples\n"
     ]
    }
   ],
   "source": [
    "# PUT DATA INTO TORCH DATALOADER\n",
    "\n",
    "print(\"INPUT + LABEL SHAPES: \", x_inputs.shape, x_masks.shape, y_labels.shape)\n",
    "\n",
    "dataset = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# USE PRE-TRAINED BERT MODEL\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels=len(authors),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kauhe\\anaconda3\\envs\\authorship-problem\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SET UP HYPERPARAMETERS\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 2 # shoudl be 2-4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.3061, 0.7895, 0.8251, 3.1632, 4.5665, 0.8240, 1.2301, 2.3222, 3.8161],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "all_labels = np.concatenate([batch[2].numpy() for batch in train_dataloader])\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',  # Option to balance automatically\n",
    "    classes=np.unique(all_labels),\n",
    "    y=all_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  1,072.    Elapsed: 0:00:02.\n",
      "  Batch    80  of  1,072.    Elapsed: 0:00:04.\n",
      "  Batch   120  of  1,072.    Elapsed: 0:00:07.\n",
      "  Batch   160  of  1,072.    Elapsed: 0:00:09.\n",
      "  Batch   200  of  1,072.    Elapsed: 0:00:11.\n",
      "  Batch   240  of  1,072.    Elapsed: 0:00:13.\n",
      "  Batch   280  of  1,072.    Elapsed: 0:00:15.\n",
      "  Batch   320  of  1,072.    Elapsed: 0:00:17.\n",
      "  Batch   360  of  1,072.    Elapsed: 0:00:19.\n",
      "  Batch   400  of  1,072.    Elapsed: 0:00:21.\n",
      "  Batch   440  of  1,072.    Elapsed: 0:00:23.\n",
      "  Batch   480  of  1,072.    Elapsed: 0:00:25.\n",
      "  Batch   520  of  1,072.    Elapsed: 0:00:27.\n",
      "  Batch   560  of  1,072.    Elapsed: 0:00:30.\n",
      "  Batch   600  of  1,072.    Elapsed: 0:00:32.\n",
      "  Batch   640  of  1,072.    Elapsed: 0:00:34.\n",
      "  Batch   680  of  1,072.    Elapsed: 0:00:36.\n",
      "  Batch   720  of  1,072.    Elapsed: 0:00:38.\n",
      "  Batch   760  of  1,072.    Elapsed: 0:00:40.\n",
      "  Batch   800  of  1,072.    Elapsed: 0:00:42.\n",
      "  Batch   840  of  1,072.    Elapsed: 0:00:44.\n",
      "  Batch   880  of  1,072.    Elapsed: 0:00:46.\n",
      "  Batch   920  of  1,072.    Elapsed: 0:00:48.\n",
      "  Batch   960  of  1,072.    Elapsed: 0:00:50.\n",
      "  Batch 1,000  of  1,072.    Elapsed: 0:00:53.\n",
      "  Batch 1,040  of  1,072.    Elapsed: 0:00:55.\n",
      "\n",
      "  Average training loss: 1.43\n",
      "  Training epcoh took: 0:00:56\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.61\n",
      "  Validation Loss: 1.04\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  1,072.    Elapsed: 0:00:02.\n",
      "  Batch    80  of  1,072.    Elapsed: 0:00:04.\n",
      "  Batch   120  of  1,072.    Elapsed: 0:00:06.\n",
      "  Batch   160  of  1,072.    Elapsed: 0:00:08.\n",
      "  Batch   200  of  1,072.    Elapsed: 0:00:10.\n",
      "  Batch   240  of  1,072.    Elapsed: 0:00:13.\n",
      "  Batch   280  of  1,072.    Elapsed: 0:00:15.\n",
      "  Batch   320  of  1,072.    Elapsed: 0:00:17.\n",
      "  Batch   360  of  1,072.    Elapsed: 0:00:19.\n",
      "  Batch   400  of  1,072.    Elapsed: 0:00:21.\n",
      "  Batch   440  of  1,072.    Elapsed: 0:00:23.\n",
      "  Batch   480  of  1,072.    Elapsed: 0:00:25.\n",
      "  Batch   520  of  1,072.    Elapsed: 0:00:27.\n",
      "  Batch   560  of  1,072.    Elapsed: 0:00:30.\n",
      "  Batch   600  of  1,072.    Elapsed: 0:00:32.\n",
      "  Batch   640  of  1,072.    Elapsed: 0:00:34.\n",
      "  Batch   680  of  1,072.    Elapsed: 0:00:36.\n",
      "  Batch   720  of  1,072.    Elapsed: 0:00:38.\n",
      "  Batch   760  of  1,072.    Elapsed: 0:00:40.\n",
      "  Batch   800  of  1,072.    Elapsed: 0:00:42.\n",
      "  Batch   840  of  1,072.    Elapsed: 0:00:44.\n",
      "  Batch   880  of  1,072.    Elapsed: 0:00:47.\n",
      "  Batch   920  of  1,072.    Elapsed: 0:00:49.\n",
      "  Batch   960  of  1,072.    Elapsed: 0:00:51.\n",
      "  Batch 1,000  of  1,072.    Elapsed: 0:00:53.\n",
      "  Batch 1,040  of  1,072.    Elapsed: 0:00:55.\n",
      "\n",
      "  Average training loss: 0.66\n",
      "  Training epcoh took: 0:00:57\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.77\n",
      "  Validation Loss: 0.67\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:56 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "seed_val = 42\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        # loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE FINE-TUNED MODEL TO DISK\n",
    "\n",
    "torch.save(model, './torch-cache/test_13_wcls.ckpt')\n",
    "\n",
    "# model_e2_b_cw1_s128 -> 0.80\n",
    "# model_e3_b_cw1_s128 -> 0.81\n",
    "# model_e4_b_cw1_s128 -> 0.85\n",
    "\n",
    "# model_e2_b_cw1_s256 -> 0.87\n",
    "# model_e3_b_cw1_s256 -> 0.91\n",
    "# model_e4_b_cw1_s256 -> 0.92\n",
    "\n",
    "# model_e2_b_cw1_s512 -> 0.80\n",
    "# model_e3_b_cw1_s512 -> 0.90\n",
    "# model_e4_b_cw1_s512 -> 0.96\n",
    "\n",
    "# model_e4_l_cw1_s256 -> 0.93\n",
    "# model_e4_l_cw1_s128 -> ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authorship-problem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
