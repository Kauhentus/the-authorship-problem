{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kauhe\\anaconda3\\envs\\authorship-problem\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA FROM DISK\n",
    "\n",
    "x_inputs = torch.load('./torch-cache/x_inputs.pt', weights_only=True)\n",
    "x_masks = torch.load('./torch-cache/x_masks.pt', weights_only=True)\n",
    "y_labels = torch.load('./torch-cache/y_labels.pt', weights_only=True)\n",
    "\n",
    "with open('./torch-cache/authors.json', 'r') as file:\n",
    "    authors = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT + LABEL SHAPES:  torch.Size([19043, 128]) torch.Size([19043, 128]) torch.Size([19043])\n",
      "17,138 training samples\n",
      "1,905 validation samples\n"
     ]
    }
   ],
   "source": [
    "# PUT DATA INTO TORCH DATALOADER\n",
    "\n",
    "print(\"INPUT + LABEL SHAPES: \", x_inputs.shape, x_masks.shape, y_labels.shape)\n",
    "\n",
    "dataset = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  100,  1045,  3305,  2017,  2025,  2026,  2935,  1045,  2572,  5580,\n",
      "         1997,  2009,  1037, 14161, 18891,  4095,  4613, 25126,  1999,  1037,\n",
      "        13219,  4540,  2026,  2935,  2017,  2442,  2425,  2149,  2073,  1996,\n",
      "         2303,  2003,  1998,  2175,  2007,  2149,  2000,  1996,  2332,  1996,\n",
      "         2303,  2003,  2007,  1996,  2332,  2021,  1996,  2332,  2003,  2025,\n",
      "         2007,  1996,  2303,  1996,  2332,  2003,  1037,  2518,  1037,  2518,\n",
      "         2026,  2935,  3288,  2033,  2000,  2032,  5342,  4419,  1998,  2035,\n",
      "         2044,  2178,  2282,  1999,  1996,  3317,  4607,  2332,  3230,  1045,\n",
      "         2031,  2741,  2000,  6148,  2032,  1998,  2000,  2424,  1996,  2303,\n",
      "         2129,  4795,  2003,  2009,  2008,  2023,  2158,  3632,  6065,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids = batch[0]\n",
    "    b_input_mask = batch[1]\n",
    "    b_labels = batch[2]\n",
    "\n",
    "    print(b_input_ids[0])\n",
    "    print(len(b_input_ids[0]))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# USE PRE-TRAINED BERT MODEL\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels=len(authors),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kauhe\\anaconda3\\envs\\authorship-problem\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SET UP HYPERPARAMETERS\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 2 # shoudl be 2-4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.3065, 0.7804, 0.8385, 3.2440, 4.5556, 0.8283, 1.2060, 2.3081, 3.7559],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "all_labels = np.concatenate([batch[2].numpy() for batch in train_dataloader])\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',  # Option to balance automatically\n",
    "    classes=np.unique(all_labels),\n",
    "    y=all_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  1,072.    Elapsed: 0:00:02.\n",
      "  Batch    80  of  1,072.    Elapsed: 0:00:04.\n",
      "  Batch   120  of  1,072.    Elapsed: 0:00:06.\n",
      "  Batch   160  of  1,072.    Elapsed: 0:00:09.\n",
      "  Batch   200  of  1,072.    Elapsed: 0:00:11.\n",
      "  Batch   240  of  1,072.    Elapsed: 0:00:13.\n",
      "  Batch   280  of  1,072.    Elapsed: 0:00:15.\n",
      "  Batch   320  of  1,072.    Elapsed: 0:00:17.\n",
      "  Batch   360  of  1,072.    Elapsed: 0:00:19.\n",
      "  Batch   400  of  1,072.    Elapsed: 0:00:21.\n",
      "  Batch   440  of  1,072.    Elapsed: 0:00:23.\n",
      "  Batch   480  of  1,072.    Elapsed: 0:00:25.\n",
      "  Batch   520  of  1,072.    Elapsed: 0:00:27.\n",
      "  Batch   560  of  1,072.    Elapsed: 0:00:29.\n",
      "  Batch   600  of  1,072.    Elapsed: 0:00:32.\n",
      "  Batch   640  of  1,072.    Elapsed: 0:00:34.\n",
      "  Batch   680  of  1,072.    Elapsed: 0:00:36.\n",
      "  Batch   720  of  1,072.    Elapsed: 0:00:38.\n",
      "  Batch   760  of  1,072.    Elapsed: 0:00:40.\n",
      "  Batch   800  of  1,072.    Elapsed: 0:00:42.\n",
      "  Batch   840  of  1,072.    Elapsed: 0:00:44.\n",
      "  Batch   880  of  1,072.    Elapsed: 0:00:46.\n",
      "  Batch   920  of  1,072.    Elapsed: 0:00:48.\n",
      "  Batch   960  of  1,072.    Elapsed: 0:00:51.\n",
      "  Batch 1,000  of  1,072.    Elapsed: 0:00:53.\n",
      "  Batch 1,040  of  1,072.    Elapsed: 0:00:55.\n",
      "\n",
      "  Average training loss: 1.43\n",
      "  Training epcoh took: 0:00:57\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.72\n",
      "  Validation Loss: 0.87\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  1,072.    Elapsed: 0:00:02.\n",
      "  Batch    80  of  1,072.    Elapsed: 0:00:04.\n",
      "  Batch   120  of  1,072.    Elapsed: 0:00:06.\n",
      "  Batch   160  of  1,072.    Elapsed: 0:00:08.\n",
      "  Batch   200  of  1,072.    Elapsed: 0:00:11.\n",
      "  Batch   240  of  1,072.    Elapsed: 0:00:13.\n",
      "  Batch   280  of  1,072.    Elapsed: 0:00:15.\n",
      "  Batch   320  of  1,072.    Elapsed: 0:00:17.\n",
      "  Batch   360  of  1,072.    Elapsed: 0:00:19.\n",
      "  Batch   400  of  1,072.    Elapsed: 0:00:21.\n",
      "  Batch   440  of  1,072.    Elapsed: 0:00:23.\n",
      "  Batch   480  of  1,072.    Elapsed: 0:00:25.\n",
      "  Batch   520  of  1,072.    Elapsed: 0:00:28.\n",
      "  Batch   560  of  1,072.    Elapsed: 0:00:30.\n",
      "  Batch   600  of  1,072.    Elapsed: 0:00:32.\n",
      "  Batch   640  of  1,072.    Elapsed: 0:00:34.\n",
      "  Batch   680  of  1,072.    Elapsed: 0:00:36.\n",
      "  Batch   720  of  1,072.    Elapsed: 0:00:38.\n",
      "  Batch   760  of  1,072.    Elapsed: 0:00:40.\n",
      "  Batch   800  of  1,072.    Elapsed: 0:00:42.\n",
      "  Batch   840  of  1,072.    Elapsed: 0:00:44.\n",
      "  Batch   880  of  1,072.    Elapsed: 0:00:46.\n",
      "  Batch   920  of  1,072.    Elapsed: 0:00:49.\n",
      "  Batch   960  of  1,072.    Elapsed: 0:00:51.\n",
      "  Batch 1,000  of  1,072.    Elapsed: 0:00:53.\n",
      "  Batch 1,040  of  1,072.    Elapsed: 0:00:55.\n",
      "\n",
      "  Average training loss: 0.63\n",
      "  Training epcoh took: 0:00:56\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.60\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:56 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "seed_val = 42\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        # loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE FINE-TUNED MODEL TO DISK\n",
    "\n",
    "torch.save(model, './torch-cache/model_e2_b_cw1_s128.ckpt')\n",
    "# model\n",
    "# model_e4_b_cw1_s128 -> 0.85\n",
    "# model_e3_b_cw1_s128 -> 0.81\n",
    "# model_e2_b_cw1_s128 -> 0.80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authorship-problem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
